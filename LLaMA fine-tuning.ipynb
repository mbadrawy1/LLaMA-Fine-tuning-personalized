{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zft1D1s_KRfB",
        "outputId": "18e4046a-2186-42b3-f98d-038436f47bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradientai\n",
            "  Downloading gradientai-1.9.1-py3-none-any.whl (353 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/353.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/353.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.6/353.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aenum>=3.1.11 (from gradientai)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.5 (from gradientai)\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
            "Installing collected packages: aenum, pydantic, gradientai\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.4\n",
            "    Uninstalling pydantic-2.6.4:\n",
            "      Successfully uninstalled pydantic-2.6.4\n",
            "Successfully installed aenum-3.1.15 gradientai-1.9.1 pydantic-1.10.15\n"
          ]
        }
      ],
      "source": [
        "!pip install gradientai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"My_Token\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"workspace_ID\""
      ],
      "metadata": {
        "id": "cHmzzxmkKjdM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gradientai import Gradient\n",
        "\n",
        "def main():\n",
        "  with Gradient() as gradient:\n",
        "      base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\")\n",
        "\n",
        "      new_model_adapter = base_model.create_model_adapter(\n",
        "          name=\"test model 3\"\n",
        "      )\n",
        "      print(f\"Created model adapter with id {new_model_adapter.id}\")\n",
        "      sample_query = \"### Instruction: continue the next word after: I am Leo Messi \\n\\n### Response:\"\n",
        "      print(f\"Asking: {sample_query}\")\n",
        "\n",
        "      # before fine-tuning\n",
        "      completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
        "      print(f\"Generated (before fine-tune): {completion}\")\n",
        "\n",
        "      samples = [\n",
        "        { \"inputs\": \"### Instruction: continue the next word after: I am Leo Messi \\n\\n### Response: considered one of the greatest footballers of all time.\" },\n",
        "        { \"inputs\": \"### Instruction: continue the next word after: I am Cristiano Ronaldo ? \\n\\n### Response: known for his incredible athleticism and goal-scoring prowess \" },\n",
        "        { \"inputs\": \"### Instruction: continue the next word after: I am Mohamad Salah \\n\\n### Response: celebrated for his speed, skill, and contributions to both club and country\" },\n",
        "      ]\n",
        "\n",
        "      #Fine tunning\n",
        "      num_epochs = 10\n",
        "      count = 0\n",
        "      while count < num_epochs:\n",
        "          print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
        "          new_model_adapter.fine_tune(samples=samples)\n",
        "          count = count + 1\n",
        "\n",
        "      # after fine-tuning\n",
        "      completion = new_model_adapter.complete(query=sample_query, max_generated_token_count=100).generated_output\n",
        "      print(f\"Generated (after fine-tune): {completion}\")\n",
        "\n",
        "      new_model_adapter.delete()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS8LiYMUKvrO",
        "outputId": "5440e065-ae2d-4a08-c03f-f25cfd4fc90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created model adapter with id c7fcdc5f-6fbf-4cf6-8b2c-605f027084cc_model_adapter\n",
            "Asking: ### Instruction: continue the next word after: I am Leo Messi \n",
            "\n",
            "### Response:\n",
            "Generated (before fine-tune):  and I am one of the best soccer players in the world.\n",
            "Fine-tuning the model, iteration 1\n",
            "Fine-tuning the model, iteration 2\n",
            "Fine-tuning the model, iteration 3\n",
            "Fine-tuning the model, iteration 4\n",
            "Fine-tuning the model, iteration 5\n",
            "Fine-tuning the model, iteration 6\n",
            "Fine-tuning the model, iteration 7\n",
            "Fine-tuning the model, iteration 8\n",
            "Fine-tuning the model, iteration 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eBkN1LJLN0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}